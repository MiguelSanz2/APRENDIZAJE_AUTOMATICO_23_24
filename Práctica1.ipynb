{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelSanz2/APRENDIZAJE_AUTOMATICO_23_24/blob/main/Pr%C3%A1ctica1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRUPO 7:\n",
        "\n",
        "Carlos Ricardo Adaro Cacho,\n",
        "\n",
        "Juan Rivera Sánchez,\n",
        "\n",
        "Miguel Sanz Almau,"
      ],
      "metadata": {
        "id": "R176k86GbAnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUCCIÓN**\n",
        "\n",
        "En este trabajo vamos a realizar una Red Neuronal la cual permita diferenciar los diferentes tipos de ropa de la aplicación zalando.\n",
        "Para ello, desarrollaremos un código que seguidamente lo explicaremos en base a sus resultados.\n",
        "\n",
        "La creación de un modelo de Red de Neuronas Artificiales capaz de reconocer y clasificar imágenes de ropa es un problema común en el campo de la visión por computadora. En este escenario, utilizaremos el conjunto de datos Fashion-MNIST, el cual contiene 70000 imágenes de prendas de ropa en 10 categorías diferentes. El objetivo es desarrollar un modelo de clasificación de prendas de ropa en Keras, aplicando un conjunto de pasos que incluyen configuración, entrenamiento, evaluación y mejora.\n",
        "\n",
        "Lo primero que deberemos hacer será importar todas las bibliotecas necesarias y asegurse de estar utilizando TensorFlow 2.0.0 o superior.\n",
        "Los datos de entrenamiento y prueba se cargarán desde `keras.datasets.fashion_mnist`.\n",
        "\n",
        "\n",
        "Las cuestiones a tener en cuenta incluyen la importancia de analizar si los datos tienen la forma esperada, la posible necesidad de preprocesar los datos utilizando la función `keras.layers.Flatten()`, y la configuración de varios modelos de red neuronal con diferentes hiperparámetros, como el número de neuronas en la capa, la función de activación, el optimizador y el número de épocas.\n",
        "\n",
        "\n",
        "Este proyecto proporciona una oportunidad para explorar y entender la influencia de diferentes hiperparámetros en el rendimiento de una red neuronal en una tarea de clasificación de imágenes. Los resultados y análisis ayudarán a tomar decisiones más informadas en la configuración de futuros modelos de aprendizaje profundo.\n",
        "\n"
      ],
      "metadata": {
        "id": "X9FWb_4a7Qml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.-Configurar y entrenar los siguientes modelos de red de neuronas, todos con función de pérdida ‘categorical_crossentropy’, métrica de precisión y 5 épocas, analizando y reflexionando sobre los resultados"
      ],
      "metadata": {
        "id": "SCmSiJU6JLoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshape and normalize the input images\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Define a function to create and train the model\n",
        "def create_and_train_model(neurons, activation, optimizer):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    print('Test loss:', test_loss)\n",
        "    print('Test accuracy:', test_acc)\n",
        "\n",
        "# Case 1: 10 neurons, relu activation, sgd optimizer\n",
        "print('Modelo 1')\n",
        "create_and_train_model(10, 'relu', 'sgd')\n",
        "\n",
        "# Case 2: 10 neurons, relu activation, rmsprop optimizer\n",
        "print('Modelo 2')\n",
        "create_and_train_model(10, 'relu', 'rmsprop')\n",
        "\n",
        "# Case 3: 10 neurons, sigmoid activation, sgd optimizer\n",
        "print('Modelo 3')\n",
        "create_and_train_model(10, 'sigmoid', 'sgd')\n",
        "\n",
        "# Case 4: 10 neurons, sigmoid activation, rmsprop optimizer\n",
        "print('Modelo 4')\n",
        "create_and_train_model(10, 'sigmoid', 'rmsprop')\n",
        "\n",
        "# Case 5: 512 neurons, relu activation, sgd optimizer\n",
        "print('Modelo 5')\n",
        "create_and_train_model(512, 'relu', 'sgd')\n",
        "\n",
        "# Case 6: 512 neurons, relu activation, rmsprop optimizer\n",
        "print('Modelo 6')\n",
        "create_and_train_model(512, 'relu', 'rmsprop')\n",
        "\n",
        "# Case 7: 512 neurons, sigmoid activation, sgd optimizer\n",
        "print('Modelo 7')\n",
        "create_and_train_model(512, 'sigmoid', 'sgd')\n",
        "\n",
        "# Case 8: 512 neurons, sigmoid activation, rmsprop optimizer\n",
        "print('Modelo 8')\n",
        "create_and_train_model(512, 'sigmoid', 'rmsprop')"
      ],
      "metadata": {
        "id": "SNSyuWBa7V7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a894fe-ba57-4ef5-d7c3-f64609df3b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 1\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.5656 - accuracy: 0.4771\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.8887 - accuracy: 0.6841\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.7528 - accuracy: 0.7301\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6803 - accuracy: 0.7657\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.7875\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.7822\n",
            "Test loss: 0.6332998275756836\n",
            "Test accuracy: 0.7821999788284302\n",
            "Modelo 2\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 2ms/step - loss: 0.8549 - accuracy: 0.7171\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5229 - accuracy: 0.8199\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4753 - accuracy: 0.8363\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4522 - accuracy: 0.8436\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4377 - accuracy: 0.8482\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4905 - accuracy: 0.8269\n",
            "Test loss: 0.4904528856277466\n",
            "Test accuracy: 0.8269000053405762\n",
            "Modelo 3\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 2.0944 - accuracy: 0.3350\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.8047 - accuracy: 0.5631\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.6125 - accuracy: 0.6010\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.4657 - accuracy: 0.6027\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.3490 - accuracy: 0.6135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.3054 - accuracy: 0.6165\n",
            "Test loss: 1.3053799867630005\n",
            "Test accuracy: 0.6165000200271606\n",
            "Modelo 4\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.3976 - accuracy: 0.6470\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.8106 - accuracy: 0.7772\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.8102\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5426 - accuracy: 0.8255\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4977 - accuracy: 0.8367\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5134 - accuracy: 0.8258\n",
            "Test loss: 0.5133983492851257\n",
            "Test accuracy: 0.8258000016212463\n",
            "Modelo 5\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 8ms/step - loss: 1.0112 - accuracy: 0.7016\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.6554 - accuracy: 0.7922\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.5786 - accuracy: 0.8133\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.5383 - accuracy: 0.8234\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5115 - accuracy: 0.8303\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5243 - accuracy: 0.8217\n",
            "Test loss: 0.5243409872055054\n",
            "Test accuracy: 0.8216999769210815\n",
            "Modelo 6\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.5592 - accuracy: 0.8001\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3857 - accuracy: 0.8594\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3392 - accuracy: 0.8760\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3126 - accuracy: 0.8841\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2939 - accuracy: 0.8914\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3427 - accuracy: 0.8770\n",
            "Test loss: 0.3426719903945923\n",
            "Test accuracy: 0.8769999742507935\n",
            "Modelo 7\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 1.6978 - accuracy: 0.5825\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.1276 - accuracy: 0.7114\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.9254 - accuracy: 0.7321\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.8236 - accuracy: 0.7446\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.7621 - accuracy: 0.7556\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7524 - accuracy: 0.7465\n",
            "Test loss: 0.7524165511131287\n",
            "Test accuracy: 0.7465000152587891\n",
            "Modelo 8\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.6057 - accuracy: 0.7878\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4499 - accuracy: 0.8374\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4125 - accuracy: 0.8507\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3845 - accuracy: 0.8609\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3665 - accuracy: 0.8665\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4409 - accuracy: 0.8430\n",
            "Test loss: 0.4408668577671051\n",
            "Test accuracy: 0.8429999947547913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de los resultados de entrenamiento y evaluación de los ocho modelos:\n",
        "\n",
        "**Modelo 1**\n",
        "- Número de Neuronas en la primera capa: 10\n",
        "- Función de Activación: ReLU\n",
        "- Optimizador: SGD\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 78.22%\n",
        "\n",
        "**Modelo 2**\n",
        "- Número de Neuronas en la primera capa: 10\n",
        "- Función de Activación: ReLU\n",
        "- Optimizador: RMSprop\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 82.69%\n",
        "\n",
        "**Modelo 3**\n",
        "- Número de Neuronas en la primera capa: 10\n",
        "- Función de Activación: Sigmoid\n",
        "- Optimizador: SGD\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 61.65%\n",
        "\n",
        "**Modelo 4**\n",
        "- Número de Neuronas en la primera capa: 10\n",
        "- Función de Activación: Sigmoid\n",
        "- Optimizador: RMSprop\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 82.58%\n",
        "\n",
        "**Modelo 5**\n",
        "- Número de Neuronas en la primera capa: 512\n",
        "- Función de Activación: ReLU\n",
        "- Optimizador: SGD\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 82.17%\n",
        "\n",
        "**Modelo 6**\n",
        "- Número de Neuronas en la primera capa: 512\n",
        "- Función de Activación: ReLU\n",
        "- Optimizador: RMSprop\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 87.70%\n",
        "\n",
        "**Modelo 7**\n",
        "- Número de Neuronas en la primera capa: 512\n",
        "- Función de Activación: Sigmoid\n",
        "- Optimizador: SGD\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 74.65%\n",
        "\n",
        "**Modelo 8**\n",
        "- Número de Neuronas en la primera capa: 512\n",
        "- Función de Activación: Sigmoid\n",
        "- Optimizador: RMSprop\n",
        "- Precisión en el conjunto de entrenamiento después de 5 épocas: 84.30%\n",
        "\n",
        "Observaciones y conclusiones:\n",
        "\n",
        "- Los modelos 2, 4, 6 y 8, que utilizan la función de activación ReLU y el optimizador RMSprop, tienden a obtener un rendimiento más alto en el conjunto de entrenamiento como en el conjunto de prueba en comparación con los modelos que usan SGD.\n",
        "\n",
        "- Los modelos con 512 neuronas en la primera capa superan en rendimiento a los modelos con solo 10 neuronas en la primera capa. Esto quiere decir que aumentar el número de neuronas puede mejorar el rendimiento, pero también puede requerir más recursos computacionales.\n",
        "\n",
        "- La elección de la función de activación tiene un impacto significativo en el rendimiento. Los modelos que utilizan ReLU superan a los modelos que utilizan sigmoid en cuanto a la precisión.\n",
        "\n",
        "- Los modelos con RMSprop como optimizador tienden a converger más rápido y a obtener mejores resultados que los modelos con SGD.\n",
        "\n"
      ],
      "metadata": {
        "id": "x7fi0irOOM7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.-Explicar la salida de la llamada model.summary() de cada uno de los 8 casos"
      ],
      "metadata": {
        "id": "oNfZc3td7XPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Cargar Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizar y ajustar el tamaño de las imágenes\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convertir las etiquetas a categóricas\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Definir una función para crear y entrenar el modelo\n",
        "def create_and_train_model(neurons, activation, optimizer):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    model.summary()\n",
        "\n",
        "# Caso 1: 10 neuronas, relu, sgd\n",
        "create_and_train_model(10, 'relu', 'sgd')\n",
        "\n",
        "# Caso 2: 10 neuronas, relu, rmsprop\n",
        "create_and_train_model(10, 'relu', 'rmsprop')\n",
        "\n",
        "# Caso 3: 10 neuronas, sigmoid, sgd\n",
        "create_and_train_model(10, 'sigmoid', 'sgd')\n",
        "\n",
        "# Caso 4: 10 neuronas, sigmoid, rmsprop\n",
        "create_and_train_model(10, 'sigmoid', 'rmsprop')\n",
        "\n",
        "# Caso 5: 512 neuronas, relu, sgd\n",
        "create_and_train_model(512, 'relu', 'sgd')\n",
        "\n",
        "# Caso 6: 512 neuronas, relu, rmsprop\n",
        "create_and_train_model(512, 'relu', 'rmsprop')\n",
        "\n",
        "# Caso 7: 512 neuronas, sigmoid, sgd\n",
        "create_and_train_model(512, 'sigmoid', 'sgd')\n",
        "\n",
        "# Caso 8: 512 neuronas, sigmoid, rmsprop\n",
        "create_and_train_model(512, 'sigmoid', 'rmsprop')"
      ],
      "metadata": {
        "id": "iImR_GVx7amG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e0065b-04dc-47dc-8e50-dba3a52b2f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 2ms/step - loss: 1.3985 - accuracy: 0.5350\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.8516 - accuracy: 0.6898\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.7278 - accuracy: 0.7387\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.7699\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.7877\n",
            "Model: \"sequential_104\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_208 (Dense)           (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_209 (Dense)           (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 2ms/step - loss: 0.7691 - accuracy: 0.7501\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5088 - accuracy: 0.8280\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4665 - accuracy: 0.8392\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4435 - accuracy: 0.8461\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4286 - accuracy: 0.8501\n",
            "Model: \"sequential_105\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_210 (Dense)           (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_211 (Dense)           (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 2.0950 - accuracy: 0.3574\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.7753 - accuracy: 0.5992\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.5615 - accuracy: 0.6344\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.3997 - accuracy: 0.6498\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.2737 - accuracy: 0.6575\n",
            "Model: \"sequential_106\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_212 (Dense)           (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_213 (Dense)           (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.4081 - accuracy: 0.6449\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.8197 - accuracy: 0.7975\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.8198\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5233 - accuracy: 0.8306\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4814 - accuracy: 0.8369\n",
            "Model: \"sequential_107\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_214 (Dense)           (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_215 (Dense)           (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.9977 - accuracy: 0.6932\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.6560 - accuracy: 0.7900\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5799 - accuracy: 0.8118\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.5388 - accuracy: 0.8226\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.5123 - accuracy: 0.8293\n",
            "Model: \"sequential_108\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_216 (Dense)           (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_217 (Dense)           (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.5626 - accuracy: 0.7981\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3863 - accuracy: 0.8588\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3390 - accuracy: 0.8742\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3156 - accuracy: 0.8831\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2949 - accuracy: 0.8907\n",
            "Model: \"sequential_109\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_218 (Dense)           (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_219 (Dense)           (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 1.6883 - accuracy: 0.5937\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 1.1212 - accuracy: 0.7150\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.9188 - accuracy: 0.7327\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.8183 - accuracy: 0.7452\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.7576 - accuracy: 0.7541\n",
            "Model: \"sequential_110\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_220 (Dense)           (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_221 (Dense)           (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.6151 - accuracy: 0.7840\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4491 - accuracy: 0.8384\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.4118 - accuracy: 0.8512\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3862 - accuracy: 0.8588\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3667 - accuracy: 0.8665\n",
            "Model: \"sequential_111\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_222 (Dense)           (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_223 (Dense)           (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se muestra una explicación de la salida del método `model.summary()` para cada uno de los 8 casos, todos tienen en común que se entrenan durante 5 épocas:\n",
        "\n",
        "**Caso 1: 10 neuronas, relu, sgd**\n",
        "- El modelo tiene dos capas: una capa de entrada con 784 entradas (correspondientes a imágenes de 28x28 píxeles) y una capa de salida con 10 neuronas.\n",
        "- La función de activación de la capa de entrada es 'relu'.\n",
        "- La función de activación de la capa de salida es 'softmax'.\n",
        "- El modelo tiene un total de 7960 parámetros (pesos y sesgos).\n",
        "\n",
        "**Caso 2: 10 neuronas, relu, rmsprop**\n",
        "- El modelo tiene la misma arquitectura que el Caso 1, pero utiliza el optimizador 'rmsprop' en lugar de 'sgd'.\n",
        "- La información de la arquitectura y la cantidad de parámetros es la misma que en el Caso 1.\n",
        "\n",
        "**Caso 3: 10 neuronas, sigmoid, sgd**\n",
        "- Este modelo es idéntico al Caso 1 en términos de arquitectura y número de parámetros, pero utiliza la función de activación 'sigmoid' en lugar de 'relu'.\n",
        "- La información de la arquitectura es la misma que en el Caso 1.\n",
        "\n",
        "**Caso 4: 10 neuronas, sigmoid, rmsprop**\n",
        "- Similar al Caso 3, pero con el optimizador 'rmsprop'.\n",
        "- La información de la arquitectura y la cantidad de parámetros es la misma que en el Caso 3.\n",
        "\n",
        "**Caso 5: 512 neuronas, relu, sgd**\n",
        "- Este modelo tiene una arquitectura más profunda que los casos anteriores. Utiliza una capa oculta con 512 neuronas, además de la capa de entrada y salida.\n",
        "- La función de activación de la capa de entrada es 'relu', al igual que la capa oculta. La capa de salida utiliza 'softmax'.\n",
        "- El modelo tiene un total de 407,050 parámetros debido a la mayor cantidad de neuronas en la capa oculta.\n",
        "\n",
        "**Caso 6: 512 neuronas, relu, rmsprop**\n",
        "- Este caso es similar al Caso 5, pero utiliza el optimizador 'rmsprop'.\n",
        "- La información de la arquitectura y la cantidad de parámetros es la misma que en el Caso 5.\n",
        "\n",
        "**Caso 7: 512 neuronas, sigmoid, sgd**\n",
        "- Este caso es similar al Caso 5, pero con la función de activación 'sigmoid'.\n",
        "- La información de la arquitectura y la cantidad de parámetros es la misma que en el Caso 5.\n",
        "\n",
        "**Caso 8: 512 neuronas, sigmoid, rmsprop**\n",
        "- Similar al Caso 7, pero con el optimizador 'rmsprop'.\n",
        "- La información de la arquitectura y la cantidad de parámetros es la misma que en el Caso 7.\n",
        "\n",
        "En resumen, la salida de `model.summary()` muestra las configuraciones de los modelos, las funciones de activación, el número de parámetros y otros detalles importantes de cada red neuronal. Las diferencias clave entre los casos son las funciones de activación, el número de neuronas y los optimizadores utilizados. Estos aspectos pueden tener un impacto significativo en el rendimiento y la capacidad de generalización de los modelos."
      ],
      "metadata": {
        "id": "v0ieRFIaR2Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.-Analizar e interpretar los resultados del caso 2 y el 7 frente a sus originales si se\n",
        "multiplica por 5 las épocas de entrenamiento (25)"
      ],
      "metadata": {
        "id": "lds-G92X7gco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Cargar Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizar y ajustar el tamaño de las imágenes\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convertir las etiquetas las etiquetas a categóricas\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Definir una función para crear y entrenar el modelo\n",
        "def create_and_train_model(neurons, activation, optimizer, epochs):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=epochs, batch_size=128)\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    print('Test loss:', test_loss)\n",
        "    print('Test accuracy:', test_acc)\n",
        "\n",
        "# Caso 2: 10 neuronas, relu, rmsprop (25 épocas)\n",
        "create_and_train_model(10, 'relu', 'rmsprop', 25)\n",
        "\n",
        "# Caso 7: 512 neuronas, sigmoid, sgd (25 épocas)\n",
        "create_and_train_model(512, 'sigmoid', 'sgd', 25)\n"
      ],
      "metadata": {
        "id": "8mIaARw-7gNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d889a9d-316d-4e55-d22d-1f985ce58a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.7946 - accuracy: 0.7383\n",
            "Epoch 2/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.8206\n",
            "Epoch 3/25\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4790 - accuracy: 0.8348\n",
            "Epoch 4/25\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4564 - accuracy: 0.8425\n",
            "Epoch 5/25\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4406 - accuracy: 0.8470\n",
            "Epoch 6/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4309 - accuracy: 0.8510\n",
            "Epoch 7/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4212 - accuracy: 0.8536\n",
            "Epoch 8/25\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4127 - accuracy: 0.8564\n",
            "Epoch 9/25\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4069 - accuracy: 0.8594\n",
            "Epoch 10/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4013 - accuracy: 0.8612\n",
            "Epoch 11/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3958 - accuracy: 0.8613\n",
            "Epoch 12/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3916 - accuracy: 0.8641\n",
            "Epoch 13/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3878 - accuracy: 0.8645\n",
            "Epoch 14/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3841 - accuracy: 0.8659\n",
            "Epoch 15/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3809 - accuracy: 0.8681\n",
            "Epoch 16/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3771 - accuracy: 0.8686\n",
            "Epoch 17/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3753 - accuracy: 0.8687\n",
            "Epoch 18/25\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3724 - accuracy: 0.8707\n",
            "Epoch 19/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3697 - accuracy: 0.8710\n",
            "Epoch 20/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3680 - accuracy: 0.8708\n",
            "Epoch 21/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3665 - accuracy: 0.8721\n",
            "Epoch 22/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3642 - accuracy: 0.8718\n",
            "Epoch 23/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3620 - accuracy: 0.8733\n",
            "Epoch 24/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3597 - accuracy: 0.8740\n",
            "Epoch 25/25\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3588 - accuracy: 0.8743\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4261 - accuracy: 0.8517\n",
            "Test loss: 0.42605844140052795\n",
            "Test accuracy: 0.8517000079154968\n",
            "Epoch 1/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 1.6880 - accuracy: 0.5940\n",
            "Epoch 2/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 1.1220 - accuracy: 0.7105\n",
            "Epoch 3/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.9205 - accuracy: 0.7326\n",
            "Epoch 4/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.8203 - accuracy: 0.7442\n",
            "Epoch 5/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.7602 - accuracy: 0.7532\n",
            "Epoch 6/25\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.7189 - accuracy: 0.7611\n",
            "Epoch 7/25\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.6881 - accuracy: 0.7671\n",
            "Epoch 8/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6641 - accuracy: 0.7732\n",
            "Epoch 9/25\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.6441 - accuracy: 0.7791\n",
            "Epoch 10/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.6275 - accuracy: 0.7840\n",
            "Epoch 11/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.6129 - accuracy: 0.7890\n",
            "Epoch 12/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6001 - accuracy: 0.7944\n",
            "Epoch 13/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5890 - accuracy: 0.7968\n",
            "Epoch 14/25\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.5787 - accuracy: 0.8005\n",
            "Epoch 15/25\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.5696 - accuracy: 0.8035\n",
            "Epoch 16/25\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.5613 - accuracy: 0.8062\n",
            "Epoch 17/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5540 - accuracy: 0.8091\n",
            "Epoch 18/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.5471 - accuracy: 0.8108\n",
            "Epoch 19/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.5408 - accuracy: 0.8130\n",
            "Epoch 20/25\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.5349 - accuracy: 0.8152\n",
            "Epoch 21/25\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.5296 - accuracy: 0.8163\n",
            "Epoch 22/25\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5248 - accuracy: 0.8183\n",
            "Epoch 23/25\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.5200 - accuracy: 0.8199\n",
            "Epoch 24/25\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.5158 - accuracy: 0.8213\n",
            "Epoch 25/25\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.5119 - accuracy: 0.8219\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.5341 - accuracy: 0.8104\n",
            "Test loss: 0.5341393947601318\n",
            "Test accuracy: 0.8104000091552734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estos son los resultados de los modelos 2 y 7 con solo 5 épocas de entrenamiento:\n",
        "\n",
        "2 -->  0.8482 ||  7 --> 0.7556\n",
        "\n",
        "Mientras que estos son los resultados de los modelos 2 y 7 con 25 épocas de entrenamiento:\n",
        "\n",
        "2 --> 0.8743  ||  7 --> 0,8219\n",
        "\n",
        "Como se puede observar, la precisión de estos modelos aumenta respectivamente a la precisón de los mismos modelos, pero con menos épocas de entrenamiento. Esto se debe a que al aumentar su número de entrenamientos, el algoritmo mejora y dará con mayor certeza el porcentaje del output esperado.\n",
        "Además, nos damos cuenta de que el porcentaje de las 5 épocas aumenta más rápido que las de 25, ya que estas van aumentando con más cautela."
      ],
      "metadata": {
        "id": "5rU17unzS9KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4.-Evaluar cada uno de los 8 modelos comparando el rendimiento del modelo en el conjunto de datos de prueba\n",
        "\n",
        "**Modelo 1**\n",
        "\n",
        "\n",
        "Precisión en el conjunto de prueba: 78.22%\n",
        "\n",
        "**Modelo 2**\n",
        "\n",
        "Número de Neuronas en la primera capa: 10\n",
        "Función de Activación: ReLU\n",
        "Optimizador: RMSprop\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 82.69%\n",
        "Precisión en el conjunto de prueba: 82.69%\n",
        "\n",
        "**Modelo 3**\n",
        "\n",
        "Número de Neuronas en la primera capa: 10\n",
        "Función de Activación: Sigmoid\n",
        "Optimizador: SGD\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 61.65%\n",
        "Precisión en el conjunto de prueba: 61.65%\n",
        "\n",
        "**Modelo 4**\n",
        "\n",
        "Número de Neuronas en la primera capa: 10\n",
        "Función de Activación: Sigmoid\n",
        "Optimizador: RMSprop\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 82.58%\n",
        "Precisión en el conjunto de prueba: 82.58%\n",
        "\n",
        "**Modelo 5**\n",
        "\n",
        "Número de Neuronas en la primera capa: 512\n",
        "Función de Activación: ReLU\n",
        "Optimizador: SGD\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 82.17%\n",
        "Precisión en el conjunto de prueba: 82.17%\n",
        "\n",
        "**Modelo 6**\n",
        "\n",
        "Número de Neuronas en la primera capa: 512\n",
        "Función de Activación: ReLU\n",
        "Optimizador: RMSprop\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 87.70%\n",
        "Precisión en el conjunto de prueba: 87.70%\n",
        "\n",
        "**Modelo 7**\n",
        "\n",
        "Número de Neuronas en la primera capa: 512\n",
        "Función de Activación: Sigmoid\n",
        "Optimizador: SGD\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 74.65%\n",
        "Precisión en el conjunto de prueba: 74.65%\n",
        "\n",
        "**Modelo 8**\n",
        "\n",
        "Número de Neuronas en la primera capa: 512\n",
        "Función de Activación: Sigmoid\n",
        "Optimizador: RMSprop\n",
        "Precisión en el conjunto de entrenamiento después de 5 épocas: 84.30%\n",
        "Precisión en el conjunto de prueba: 84.30%\n"
      ],
      "metadata": {
        "id": "ZBBRmV1P7mPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.- Usar cada uno de los 8 modelos para hacer predicciones sobre la 6ª imagen de\n",
        "test (test_images[5])"
      ],
      "metadata": {
        "id": "vrHyVD5z7vi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Cargar Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizar y ajustar el tamaño de las imágenes\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convertir las etiquetas a categóricas\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "\n",
        "model = None\n",
        "# Definir una función para crear y entrenar el modelo\n",
        "def create_and_train_model(neurons, activation, optimizer):\n",
        "    global model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    print('Test loss:', test_loss)\n",
        "    print('Test accuracy:', test_acc)\n",
        "\n",
        "# Caso 1: 10 neuronas, relu, sgd\n",
        "print('Modelo 1')\n",
        "create_and_train_model(10, 'relu', 'sgd')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 2: 10 neuronas, relu, rmsprop\n",
        "print('Modelo 2')\n",
        "create_and_train_model(10, 'relu', 'rmsprop')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 3: 10 neuronas, sigmoid, sgd\n",
        "print('Modelo 3')\n",
        "create_and_train_model(10, 'sigmoid', 'sgd')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 4: 10 neuronas, sigmoid, rmsprop\n",
        "print('Modelo 4')\n",
        "create_and_train_model(10, 'sigmoid', 'rmsprop')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 5: 512 neuronas, relu, sgd\n",
        "print('Modelo 5')\n",
        "create_and_train_model(512, 'relu', 'sgd')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 6: 512 neuronas, relu, rmsprop\n",
        "print('Modelo 6')\n",
        "create_and_train_model(512, 'relu', 'rmsprop')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 7: 512 neuronas, sigmoid, sgd\n",
        "print('Modelo 7')\n",
        "create_and_train_model(512, 'sigmoid', 'sgd')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n",
        "\n",
        "# Caso 8: 512 neuronas, sigmoid, rmsprop\n",
        "print('Modelo 8')\n",
        "create_and_train_model(512, 'sigmoid', 'rmsprop')\n",
        "# Obtener las predicciones del modelo para la 6ª imagen de prueba\n",
        "predictions = model.predict(test_images)\n",
        "predictions_array = predictions[5]\n",
        "true_label = test_labels[5]\n",
        "img = test_images[5]\n"
      ],
      "metadata": {
        "id": "HamyIjN07yb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad93e8ae-2f36-418b-8d1a-37bfc551b52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 1\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 6ms/step - loss: 1.4209 - accuracy: 0.5182\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.8441 - accuracy: 0.7119\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.7500\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.7719\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.6130 - accuracy: 0.7872\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.7776\n",
            "Test loss: 0.6257163286209106\n",
            "Test accuracy: 0.7775999903678894\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Modelo 2\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.8354 - accuracy: 0.7244\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5321 - accuracy: 0.8194\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4844 - accuracy: 0.8339\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4587 - accuracy: 0.8423\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4402 - accuracy: 0.8484\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4646 - accuracy: 0.8422\n",
            "Test loss: 0.46458643674850464\n",
            "Test accuracy: 0.842199981212616\n",
            "313/313 [==============================] - 1s 1ms/step\n",
            "Modelo 3\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 2.1229 - accuracy: 0.3724\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.8622 - accuracy: 0.5082\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.6463 - accuracy: 0.5648\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.4740 - accuracy: 0.6126\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.3414 - accuracy: 0.6287\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2918 - accuracy: 0.6305\n",
            "Test loss: 1.2918365001678467\n",
            "Test accuracy: 0.6305000185966492\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Modelo 4\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.4273 - accuracy: 0.6298\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.8347 - accuracy: 0.7843\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.6291 - accuracy: 0.8116\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.5438 - accuracy: 0.8249\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4991 - accuracy: 0.8333\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 0.5167 - accuracy: 0.8230\n",
            "Test loss: 0.5167158246040344\n",
            "Test accuracy: 0.8230000138282776\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Modelo 5\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 1.0034 - accuracy: 0.6943\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.6564 - accuracy: 0.7891\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5790 - accuracy: 0.8135\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5375 - accuracy: 0.8237\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.5111 - accuracy: 0.8308\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5280 - accuracy: 0.8200\n",
            "Test loss: 0.5280272364616394\n",
            "Test accuracy: 0.8199999928474426\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "Modelo 6\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.5607 - accuracy: 0.7995\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.3852 - accuracy: 0.8595\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.3397 - accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3137 - accuracy: 0.8841\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2929 - accuracy: 0.8909\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3478 - accuracy: 0.8751\n",
            "Test loss: 0.34780094027519226\n",
            "Test accuracy: 0.8751000165939331\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "Modelo 7\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 1.7105 - accuracy: 0.5999\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.1335 - accuracy: 0.7151\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.9274 - accuracy: 0.7321\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.8246 - accuracy: 0.7455\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.7623 - accuracy: 0.7542\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.7530 - accuracy: 0.7513\n",
            "Test loss: 0.7530409097671509\n",
            "Test accuracy: 0.7512999773025513\n",
            "313/313 [==============================] - 3s 8ms/step\n",
            "Modelo 8\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.6109 - accuracy: 0.7844\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.4493 - accuracy: 0.8378\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.4129 - accuracy: 0.8505\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.3868 - accuracy: 0.8593\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3669 - accuracy: 0.8660\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4120 - accuracy: 0.8503\n",
            "Test loss: 0.4120029807090759\n",
            "Test accuracy: 0.8503000140190125\n",
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora analizaremos los resultados para cada modelo:\n",
        "\n",
        "**Modelo 1**\n",
        "- Accuracy en el conjunto de prueba: 0.7776\n",
        "- Loss en el conjunto de prueba: 0.6257\n",
        "\n",
        "En este modelo, se utilizaron 10 neuronas en la capa oculta, activación ReLU y optimizador SGD. El modelo logró un accuracy en el conjunto de prueba del 77.76%. Esto significa que aproximadamente el 77.76% de las imágenes de prueba se clasificaron correctamente. El valor de pérdida en el conjunto de prueba fue 0.6257, lo que representa la magnitud de los errores en las predicciones.\n",
        "\n",
        "**Modelo 2**\n",
        "- Accuracy en el conjunto de prueba: 0.8422\n",
        "- Loss en el conjunto de prueba: 0.4646\n",
        "\n",
        "En este modelo, también se utilizaron 10 neuronas en la capa oculta, activación ReLU, pero con el optimizador RMSprop. El accuracy en el conjunto de prueba aumentó significativamente a 0.8422, lo que indica una mejora en el rendimiento en comparación con el Modelo 1. El valor de pérdida en el conjunto de prueba disminuyó a 0.4646, lo que sugiere que el modelo está haciendo predicciones más precisas.\n",
        "\n",
        "**Modelo 3**\n",
        "- Accuracy en el conjunto de prueba: 0.6305\n",
        "- Loss en el conjunto de prueba: 1.2918\n",
        "\n",
        "En este modelo, se utilizaron 10 neuronas en la capa oculta, activación sigmoid y optimizador SGD. El accuracy en el conjunto de prueba es inferior (0.6305) en comparación con los dos primeros modelos. El valor de pérdida es más alto (1.2918), lo que indica que este modelo tiene dificultades para hacer predicciones precisas.\n",
        "\n",
        "**Modelo 4**\n",
        "- Accuracy en el conjunto de prueba: 0.8230\n",
        "- Loss en el conjunto de prueba: 0.5167\n",
        "\n",
        "En este modelo, se utilizaron 10 neuronas en la capa oculta, activación sigmoid y optimizador RMSprop. El modelo muestra una mejora en el accuracy (0.8230) en comparación con el Modelo 3, y la pérdida en el conjunto de prueba disminuyó a 0.5167.\n",
        "\n",
        "**Modelo 5**\n",
        "- Accuracy en el conjunto de prueba: 0.8200\n",
        "- Loss en el conjunto de prueba: 0.5280\n",
        "\n",
        "Este modelo utilizó 512 neuronas en la capa oculta, activación ReLU y optimizador SGD. El accuracy (0.8200) es similar al del Modelo 4, pero el modelo tiene una mayor complejidad debido al mayor número de neuronas en la capa oculta.\n",
        "\n",
        "**Modelo 6**\n",
        "- Accuracy en el conjunto de prueba: 0.8751\n",
        "- Loss en el conjunto de prueba: 0.3478\n",
        "\n",
        "Este modelo también utilizó 512 neuronas en la capa oculta, activación ReLU, pero con el optimizador RMSprop. Logró un accuracy superior (0.8751) en comparación con el Modelo 5. La pérdida en el conjunto de prueba es más baja (0.3478), lo que indica predicciones más precisas.\n",
        "\n",
        "**Modelo 7**\n",
        "- Accuracy en el conjunto de prueba: 0.7513\n",
        "- Loss en el conjunto de prueba: 0.7530\n",
        "\n",
        "En este modelo, se utilizaron 512 neuronas en la capa oculta, activación sigmoid y optimizador SGD. El accuracy (0.7513) es inferior en comparación con los Modelos 5 y 6. El valor de pérdida en el conjunto de prueba es alto (0.7530), lo que sugiere que el modelo no está funcionando tan bien como otros.\n",
        "\n",
        "**Modelo 8**\n",
        "- Accuracy en el conjunto de prueba: 0.8503\n",
        "- Loss en el conjunto de prueba: 0.4120\n",
        "\n",
        "Este modelo también utilizó 512 neuronas en la capa oculta, activación sigmoid, pero con el optimizador RMSprop. Obtuvo un accuracy de 0.8503 en el conjunto de prueba, lo que es un rendimiento bastante bueno. La pérdida en el conjunto de prueba es de 0.4120, lo que indica una buena precisión en las predicciones.\n",
        "\n",
        "En general, los resultados sugieren que los modelos con activación ReLU y optimizadores RMSprop tienden a funcionar mejor que los modelos con activación sigmoid y optimizador SGD. Los modelos más complejos con 512 neuronas parecen tener un mejor rendimiento que los modelos con solo 10 neuronas. Sin embargo, el rendimiento puede variar según la configuración específica del modelo y la naturaleza del conjunto de datos. Es importante considerar no solo el accuracy sino también la pérdida y la complejidad del modelo al evaluar su rendimiento."
      ],
      "metadata": {
        "id": "1orEg5ghXB1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.- Utilícelo para ver la predicción, igual que el gráfico anterior, de las 10 primeras\n",
        "imágenes del conjunto de test, para cada uno de los 8 casos. Reflexione y\n",
        "comente las diferencias que observa."
      ],
      "metadata": {
        "id": "01R8KNnd7yGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Cargar Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizar y ajustar el tamaño de las imágenes\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convertir las etiquetas a categóricas\n",
        "train_labels = keras.utils.to_categorical(train_labels)\n",
        "test_labels = keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Definir una lista con las diferentes clases\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "#Usar las funciones dadas\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array, int(true_label[i]), img\n",
        "    img = img.reshape(28, 28) #Debimos modificar la función, ya que nos daba un error de tamaño\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         class_names[true_label]),\n",
        "                                         color=color)\n",
        "\n",
        "def plot_value_array(a, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array, int(true_label[0])  # Convert true_label to an integer\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(10))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('green')\n",
        "\n",
        "# Definir una función para crear y entrenar el modelo\n",
        "def create_and_train_model(neurons, activation, optimizer):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    model.summary()\n",
        "    predictions = model.predict(test_images)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "#Crear un bucle que imprima las 10 primeras imagenes y sus gráficos\n",
        "    for j in range(10):\n",
        "            plt.subplot(2, 10, j + 1)\n",
        "            plot_image(j, predictions[j], test_labels[j], test_images[j])\n",
        "            plt.subplot(2, 10, j + 11)\n",
        "            plot_value_array(j, predictions[j], test_labels[j])\n",
        "    plt.show()\n",
        "\n",
        "# Case 1: 10 neuronas, relu , sgd\n",
        "print('Modelo 1')\n",
        "create_and_train_model(10, 'relu', 'sgd')\n",
        "\n",
        "# Case 2: 10 neuronas, relu , rmsprop\n",
        "print('Modelo 2')\n",
        "create_and_train_model(10, 'relu', 'rmsprop')\n",
        "\n",
        "# Case 3: 10 neuronas, sigmoid , sgd\n",
        "print('Modelo 3')\n",
        "create_and_train_model(10, 'sigmoid', 'sgd')\n",
        "\n",
        "# Case 4: 10 neuronas, sigmoid , rmsprop\n",
        "print('Modelo 4')\n",
        "create_and_train_model(10, 'sigmoid', 'rmsprop')\n",
        "\n",
        "# Case 5: 512 neuronas, relu , sgd\n",
        "print('Modelo 5')\n",
        "create_and_train_model(512, 'relu', 'sgd')\n",
        "\n",
        "# Case 6: 512 neuronas, relu , rmsprop\n",
        "print('Modelo 6')\n",
        "create_and_train_model(512, 'relu', 'rmsprop')\n",
        "\n",
        "# Case 7: 512 neuronas, sigmoid , sgd\n",
        "print('Modelo 7')\n",
        "create_and_train_model(512, 'sigmoid', 'sgd')\n",
        "\n",
        "# Case 8: 512 neuronas, sigmoid , rmsprop\n",
        "print('Modelo 8')\n",
        "create_and_train_model(512, 'sigmoid', 'rmsprop')\n"
      ],
      "metadata": {
        "id": "xcihryJS75Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las diferencias que se pueden observar en los resultados de los diferentes modelos son principalmente en términos de precisión (accuracy) y velocidad de entrenamiento.\n",
        "1. **Precisión en el conjunto de prueba (Test Accuracy)**:\n",
        "    - Modelo 1: Tiene una precisión de alrededor del 77.76%.\n",
        "    - Modelo 2: Obtiene una precisión de aproximadamente el 84.22%.\n",
        "    - Modelo 3: Alcanza una precisión de aproximadamente el 63.05%.\n",
        "    - Modelo 4: Logra una precisión de alrededor del 82.30%.\n",
        "    - Modelo 5: Tiene una precisión de aproximadamente el 81.99%.\n",
        "    - Modelo 6: Obtiene una precisión de alrededor del 87.51%.\n",
        "    - Modelo 7: Alcanza una precisión de aproximadamente el 75.13%.\n",
        "    - Modelo 8: Logra una precisión de aproximadamente el 85.03%.\n",
        "\n",
        "2. **Velocidad de entrenamiento**:\n",
        "    - Modelos 1 a 4 son más rápidos de entrenar, con tiempos de entrenamiento por época que van desde 1 a 2 segundos. Esto es probablemente porque tienen menos neuronas y, por lo tanto, menos parámetros en la red.\n",
        "    - Modelos 5 a 8 son más lentos de entrenar, con tiempos de entrenamiento por época que van desde 4 a 6 segundos. Esto se debe a que tienen una capa densa con 512 neuronas, lo que resulta en más parámetros y cálculos más intensivos.\n",
        "\n",
        "3. **Arquitectura de la red**:\n",
        "    - Todos los modelos tienen una arquitectura similar con una capa de entrada y una capa de salida con activación softmax. La única diferencia radica en el número de neuronas en la capa oculta y la función de activación utilizada.\n",
        "\n",
        "4. **Función de activación y Optimizador**:\n",
        "    - Se han probado dos funciones de activación, 'relu' y 'sigmoid', en combinación con dos optimizadores, 'sgd' y 'rmsprop'.\n",
        "    - Los modelos 1 y 2 utilizan la función de activación 'relu', mientras que los modelos 3 y 4 utilizan 'sigmoid'. En términos de optimizadores, los modelos 1 y 3 utilizan 'sgd', y los modelos 2 y 4 utilizan 'rmsprop'.\n",
        "    - Los modelos 5 a 8 tienen una arquitectura similar a los modelos 1 a 4, pero con 512 neuronas en la capa oculta en lugar de 10.\n",
        "\n",
        "En general, se puede observar que los modelos con 512 neuronas en la capa oculta tienden a tener una mejor precisión en el conjunto de prueba, pero también tienen un tiempo de entrenamiento más largo. Además, los modelos con activación 'relu' tienden a funcionar mejor que los modelos con activación 'sigmoid' en este conjunto de datos específico.\n",
        "\n",
        "La elección de la arquitectura de red, la función de activación y el optimizador depende de las necesidades y restricciones específicas del problema, como la cantidad de datos disponibles y el tiempo disponible para entrenar el modelo."
      ],
      "metadata": {
        "id": "vlF898_ib9M6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.- Mejore el modelo usando el de la configuración del caso 3, pero cambiando el\n",
        "optimizador por ‘adam’ y la función de pérdida\n",
        "‘sparse_categorical_crossentropy’. Buscar en internet las bases de dicho\n",
        "optimizador y función de pérdida, explícalos con tus propias palabras y plantea\n",
        "tus reflexiones respecto al resultado."
      ],
      "metadata": {
        "id": "5xpH5k-47xlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Normalizar y ajustar el tamaño de las imágenes\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Convertir las etiquetas a categóricas\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Definir una función para crear y entrenar el modelo\n",
        "def create_and_train_model(neurons, activation, optimizer):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(neurons, activation=activation, input_shape=(28*28,)))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    print('Test loss:', test_loss)\n",
        "    print('Test accuracy:', test_acc)\n",
        "\n",
        "\n",
        "print('Modelo con Adam  and sparse_categorical_crossentropy ')\n",
        "create_and_train_model(10, 'sigmoid', 'adam')"
      ],
      "metadata": {
        "id": "Bwuq2zDd7Jkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece8bc68-396c-4419-b278-09bb3129b9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with Adam optimizer and sparse_categorical_crossentropy loss\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.4567 - accuracy: 0.6047\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.8964 - accuracy: 0.7600\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.8045\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5978 - accuracy: 0.8222\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5379 - accuracy: 0.8317\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5470 - accuracy: 0.8209\n",
            "Test loss: 0.5469821095466614\n",
            "Test accuracy: 0.820900022983551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizador Adaptativo \"Adam\":\n",
        "\n",
        "El optimizador \"Adam\", que significa Adaptive Moment Estimation, es un algoritmo ampliamente empleado en el entrenamiento de redes neuronales. Su característica principal radica en su habilidad para ajustar la tasa de aprendizaje automáticamente durante el proceso de entrenamiento. Esto se logra mediante el cálculo de promedios móviles de primer y segundo orden de los gradientes de los pesos de la red.\n",
        "\n",
        "En resumen, \"Adam\" es un optimizador eficaz y eficiente para la adaptación de los pesos de la red, lo que acelera la convergencia y resuelve problemas de ajuste más rápidamente en comparación con algoritmos más simples, como el descenso de gradiente estocástico (SGD).\n",
        "\n",
        "Función de Pérdida \"Sparse Categorical Crossentropy\":\n",
        "\n",
        "La función de pérdida \"sparse_categorical_crossentropy\" se utiliza comúnmente en tareas de clasificación con múltiples clases, en las cuales las etiquetas se representan como enteros en lugar de codificaciones \"one-hot.\" Esta función de pérdida calcula la entropía cruzada entre las distribuciones de probabilidad previstas por el modelo y las etiquetas reales.\n",
        "\n",
        "A diferencia de otras funciones de pérdida, que requieren representaciones \"one-hot\" para las etiquetas, \"sparse_categorical_crossentropy\" permite utilizar simples enteros para las etiquetas, lo que facilita la implementación y mejora la eficiencia.\n",
        "\n",
        "Reflexiones sobre los Resultados:\n",
        "\n",
        "Cuando se ajustan el optimizador y la función de pérdida en un modelo, es esencial considerar que esto puede influir en los resultados.\n",
        "1. Velocidad de Convergencia: \"Adam\" a menudo converge más rápidamente en comparación con otros optimizadores, como \"RMSprop.\" Esto puede resultar en tiempos de entrenamiento más cortos.\n",
        "\n",
        "2. Estabilidad del Entrenamiento: La elección de \"sparse_categorical_crossentropy\" es beneficiosa en problemas de clasificación de múltiples clases, especialmente cuando las etiquetas se representan como enteros. Esto puede mejorar la estabilidad y eficiencia del entrenamiento.\n",
        "\n",
        "3. Rendimiento Final: Los cambios en el optimizador y la función de pérdida pueden tener un impacto en el rendimiento final del modelo en el conjunto de datos de prueba. Evaluar y comparar los resultados con la configuración previa es crucial para determinar si la nueva configuración es una mejora real.\n",
        "En general, la elección del optimizador y la función de pérdida depende del problema que se nos plantea. Experimentar con diferentes configuraciones es esencial para determinar cuál funciona mejor en un caso particular. Es fundamental mantener un seguimiento y evaluación continuos de los resultados para realizar ajustes y mejoras según sea necesario."
      ],
      "metadata": {
        "id": "0ceaEylFfEHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusión, en este proyecto, hemos configurado y entrenado con éxito una red neuronal para clasificar prendas de vestir del conjunto de datos Fashion MNIST. Hemos logrado una precisión razonable y hemos evaluado el rendimiento del modelo en datos de prueba.\n",
        "\n",
        " Las reflexiones y comparaciones realizadas contribuyen a una mejor comprensión de cómo sintonizar y mejorar el rendimiento de los modelos de redes neuronales en aplicaciones prácticas."
      ],
      "metadata": {
        "id": "vEqfscR-gt3X"
      }
    }
  ]
}